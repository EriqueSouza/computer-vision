{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we begin\n",
    "\n",
    "This material assumes that you have an introductory level of neural networks. If you feel you need to reinforce your knowledge on these subjects, take a look into our [Deep learning foundation course materials](https://github.com/sebratec-academy/deep-learning-foundation), and consider enrolling into Sebratec academy! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can be applied to a lot of different challenges, and image recognition is one of them. In this first part of the material, you will be introduced to a particular neural network architecture called convolutional neural network, or CNN, in short. This architecture is really good in exploring properties of image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of an image\n",
    "\n",
    "Before we can explore how CNNs can take advantage of these properties of images, we need to talk about what an image is, and about its properties. Images are composed of pixels, made from one or more measurements of light, and these pixels are the smallest elements present in a screen.\n",
    "\n",
    "\n",
    "Consider a black and white image. It can be represented into a 2D array of numerical values. Each index of this array, in a black and white image, holds a value between 0 to 255, where 0 is black, 255 is white, and the numbers in between are shades of gray.\n",
    "\n",
    "![bw-pixels](https://user-images.githubusercontent.com/20716798/78840184-64507200-79fa-11ea-8117-6660e489a3e0.png)\n",
    "\n",
    "\n",
    "For RGB images, it is a little different. There, the pixels are arranged in a 3D array. The first two dimensions represent the position of the pixels that form the image, and the colors are stored inside the third dimension. The third dimension has 3 layers, each corresponding to an RGB color. So, each pixel has three values, one for the amount of red, one for the amount of green, and one for the amount of blue.\n",
    "\n",
    "![rgb-pixels](https://user-images.githubusercontent.com/20716798/78840721-d70e1d00-79fb-11ea-9b30-180b2be0f629.png)\n",
    "\n",
    "\n",
    "These layers are overlayed, and the final pixel is composed of the mix of these three color values. So, if you take for example a pixel with (255, 125, 80) RGB values and the final mix would look like this:\n",
    "\n",
    "![mixed colors](https://user-images.githubusercontent.com/20716798/77757571-ba540b80-7031-11ea-9017-56fa4059327d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding an image to a neural network\n",
    "\n",
    "So, in the end, an image is a large 3D array of numbers. If your image has a dimension of 512 * 512, for example, you would have 786.432 numbers inside it.\n",
    "\n",
    "*to find this number, just multiply the dimensions. In this case, 512 * 512 * 3*\n",
    "\n",
    "Each of these numbers is an individual feature of the image. Remember that the number of parameters of a neuron in a neural network is the number features and bias, so our neural net, in this case, would have 786.432 + 1 parameters, and that is a lot of parameters for a neural network, that not only is inefficient but also can lead to overfitting very quickly.\n",
    "\n",
    "Let's say you are trying to recognize something in an image. For example, a car. The car can be located at any point between these 786.432 parameters, and every car image can be very different from one another. The car can be in different positions, in different scenarios, with or without noise in the image, from different angles, with different lighting, and if you analyze these images represented as data, they will be very different from each other, but a car will still be there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How CNNs help to address these concerns\n",
    "\n",
    "To address these concerns, we will use a convolutional neural network, in short, CNN, which is a special kind of neural network that can find useful patterns in an image. As mentioned above, many neural networks are fed with individual inputs, and since each color value inside each pixel is a different input, it would take too much time to learn from these inputs, and they can also lead to overfitting.  \n",
    "\n",
    "CNNs are useful in this case because instead of using every single input from our image, it uses a technique called parameter sharing, which applies a filter, also called a convolutional kernel, to groups of pixels in different areas of an image, instead of analyzing the whole image at the same time. \n",
    "\n",
    "Every CNN is composed of multiple layers, and these layers usually are convolutional, pooling and fully connected layers. Each produces an output that is used for the next one to produce your expected result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convolutional layer\n",
    "\n",
    "The first important layer of a convolutional neural network is called the convolutional layer. This layer is defined by the filter size, stride, depth and padding parameters, and is the layer inside a CNN responsible for applying the filter to the image. These filters are usually small grids of values that slide over an image, pixel by pixel and outputs a filtered image exposing the extracted feature. The resulting image will be about the same size as the original image.\n",
    "\n",
    "![applying-filter](https://user-images.githubusercontent.com/20716798/79057315-76cbe500-7c60-11ea-8dc7-6a30dc25f2da.gif)\n",
    "\n",
    "\n",
    "Under the hood, the filter works by applying convolution to the image, pixel by pixel.\n",
    "\n",
    "The convolution works by:\n",
    "\n",
    "- Multiplying the values in the filter with their matching pixel value. So, the value in the top left of our filter (0), will be multiplied by the pixel value in that same corner in our image area (7).\n",
    "\n",
    "- Sum all these multiplied pairs of values to get a new value, in this case, 9. This value will be the new pixel value in the filtered output image, at the same location as the selected center pixel.\n",
    "\n",
    "\n",
    "![3D_Convolution_Animation](https://user-images.githubusercontent.com/20716798/78149745-a136ca80-7436-11ea-9097-fbb6b99e8cf1.gif)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{bmatrix}\n",
    "      0  &  -1  &  0  \\\\\n",
    "      -1  &  5  &  -1  \\\\\n",
    "      0  &  -1  &  0 \\\\\n",
    "  \\end{bmatrix}\n",
    "  .  \n",
    "  \\begin{bmatrix}\n",
    "    7  &  7  &  6  \\\\\n",
    "    7  &  7  &  6  \\\\\n",
    "    6  &  6  &  4 \\\\\n",
    "  \\end{bmatrix}\n",
    "  =\n",
    "  \\sum\n",
    "  \\begin{bmatrix}\n",
    "    0  &  -7  &  0  \\\\\n",
    "    -7  &  35  &  -6  \\\\\n",
    "    0  &  -6  &  0 \\\\\n",
    "  \\end{bmatrix}\n",
    "  =\n",
    "  9\n",
    "\\end{equation}\n",
    "\n",
    "It is also important to note that these values inside the filters are called weights. The weights determine how important the pixel is when forming the output image. In our example, the center weight is five, meaning that the center pixel is the most important one in our filter.\n",
    "\n",
    "\n",
    "The filter has a size, which corresponds to how many inputs features in the width and height dimensions one neuron takes in. We do not split up the image by its depth (or the channels), only the width and height. So if we specify the filter size, the number of inputs that our filter will take is filter_width * filter_height * input_depth + 1.\n",
    "\n",
    "\n",
    "![filter](https://user-images.githubusercontent.com/20716798/77867552-083a6080-7238-11ea-8ebb-b5318fb03d52.png)\n",
    "\n",
    "And to apply this filter to an image that is bigger than itself, we must move it. We specify how many pixels the filter is going to move with the **stride** hyperparameter. When the stride is 4, for example, then the filters jump 4 pixels at a time as we slide them around. Having a larger stride will produce smaller feature maps.\n",
    "\n",
    "\n",
    "![stride](https://user-images.githubusercontent.com/20716798/77869803-12f8f380-7240-11ea-8b36-965114ff669b.gif)\n",
    "\n",
    "You can also use multiple filters instead of just one. The amount of these filters is the hyperparameter called **depth**. These filters produce filtered images based on the input image, extracting a feature from this image that we can use to achieve our desired result. Our filters can, for example, extract the edges from our original image, others can detect useful color patterns and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the filter will not entirely fit inside the image, especially when close to the border. To prevent this, we can specify the **padding** hyperparameter, which in most cases, will ignore or add zero to the missing pixels.\n",
    "\n",
    "![padding](https://user-images.githubusercontent.com/20716798/79000066-3f6c1400-7b4c-11ea-833c-4f5545147595.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Remember that our pixel values range from 0 to 255? Before we feed these pixels into the convolutional layer, it is important to normalize them to a range between 0 and 1. So, in practice, a black pixel would still be 0, a white pixel would be 1, and a gray pixel with a 145 value would be converted into something close to 0.56 \n",
    "\n",
    "![normalization2](https://user-images.githubusercontent.com/20716798/79002726-5c571600-7b51-11ea-9229-000a7cc01100.png)\n",
    "\n",
    "## Activation functions\n",
    "\n",
    "The convolutional layer is often paired up with the relu activation function. The relu function is pretty simple, it returns zero if the input is less than or equal to zero, and the input itself if it's not. It is a very good function for this specific job because there are occasions where our convolutional filter will be applied to some pixels, and the output will be a negative number. Zero is as dark as our pixel can get, there are no pixels darker than pure black. So, if our output is lower than zero, we just convert it back to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [6,2.5]\n",
    "import numpy as np\n",
    "\n",
    "# x domain: 41 samples from -10 to 10\n",
    "x = np.linspace(-10, 10, 41)\n",
    "x1  = np.linspace(-10, 10, 41)\n",
    "\n",
    "# ReLU function (Rectified Linear Unit)\n",
    "# Ranges from 0 to inf\n",
    "def ReLU(x):\n",
    "  y = [0 if  i < 0 else i for i in x]\n",
    "  return y\n",
    "\n",
    "# Plot the activation function\n",
    "img = mpimg.imread(\"https://user-images.githubusercontent.com/20716798/79173890-3eabda00-7df9-11ea-9963-28561dc29e97.png\")\n",
    "f, axarr = plt.subplots(1,2, gridspec_kw={'width_ratios': [0.8, 1]})\n",
    "axarr[1].plot(x, ReLU(x), 'tab:orange', label='ReLU')\n",
    "axarr[0].imshow(img)\n",
    "\n",
    "axarr[1].grid()\n",
    "axarr[1].legend()\n",
    "axarr[0].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layer\n",
    "\n",
    "Following the convolutional layer, comes the pooling layer. The pooling layer is very useful inside our architecture because as our input image moves through our convolutional layers, we are converting an image into a series of stacks of feature maps about the same size as our input image. The convolutional layer does a pretty good job of extracting our desired features, but its outputs are still very large.\n",
    "\n",
    "This large amount of information has some downsides:\n",
    "\n",
    "- They are still very prone to overfitting because they record the precise position of the features in the feature map. This means that small variations between the feature in the input image will result in a very different feature map. These variations can happen with different lightning, positioning, shifting, rotation, cropping and that list goes on;\n",
    "\n",
    "- A large amount of information means a lot of computational power and time to process this information.\n",
    "\n",
    "So, the purpose of the pooling layer is to reduce the spatial size (width and height) of the layers and it does not touch on the depth. This reduces the number of parameters required in future layers after this pooling layer.\n",
    "\n",
    "There are a few types of pooling layers, the max-pooling layer being one of the most common.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling layer\n",
    "\n",
    "The max-pooling layer takes a stack of feature maps as an input. Like the convolutional layers, we must also specify size and stride. It is common to choose the window size 2x2 and stride as 2 for these hyperparameters. \n",
    "\n",
    "The max-pooling layer reduces the size of the feature maps by taking the maximum values from the inputs inside our window. Then, it does the same for all of the inputs inside our feature maps.\n",
    "\n",
    "![Max_pooling](https://user-images.githubusercontent.com/20716798/79089296-f54e8280-7d45-11ea-87fd-4176725b8638.png)\n",
    "\n",
    "As described in the image above, our max-pooling layer:\n",
    "\n",
    "- Receives a 4x4 feature map;\n",
    "- Uses a 2x2 window size, and a stride of 2;\n",
    "- Inside the first window, there are the values [1,0,4,6];\n",
    "- The highest value among these values is 6, so this is the output of the first pooling operation;\n",
    "- Moves the window based on the stride value. In this case, 2;\n",
    "- Repeats until the whole feature map is reduced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected layer\n",
    "\n",
    "The fully connected layer is usually the last layer inside our convolutional neural network. It is the same as a standard dense neural network layer, meaning that every neuron in the next layer takes as input every neuron in the previous layer’s output. So, taking our example from above, if we have a 4x4x1 input, we would have 16 inputs to it.\n",
    "\n",
    "The role of our fully connected layer is to predict a class to our input image, and the number of nodes inside our fully connected layer is the number of classes that it can choose from. So, for example, if your network can classify an image as a car or not car, it would have two nodes inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
